{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab6201d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully cleaned and saved to 'FA_Data_Cleaned.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# This simulates loading your data.\n",
    "# In your real code, you would use: df = pd.read_csv(\"your_file.csv\")\n",
    "data = {\n",
    "    'วันครบอายุเห็นชอบ': [\n",
    "        '2/6/2562', '4/19/2562', '2/6/2562', '___', '5/20/2562', '6/10/2562',\n",
    "        '6/24/2562', '8/3/2562', '9/1/2562', '9/4/2562', '11/4/2562', '10/19/2562',\n",
    "        '9/24/2562', '11/29/2562', '11/27/2562', '12/7/2562', '3/30/2563', '4/7/2563',\n",
    "        '5/1/2563', '4/20/2563', '4/17/2563', '5/5/2563', '5/24/2563', '7/14/2563',\n",
    "        '6/5/2563', '7/6/2563', 'nan', '8/10/2563', '9/4/2563', '8/10/2563', 'nan',\n",
    "        '10/10/2563', '9/30/2563', '12/9/2563', '1/11/2564', '1/20/2564', '2/14/2564',\n",
    "        '2/27/2564', '4/4/2564', '3/3/2564', '4/21/2564', '5/17/2564', '5/22/2564',\n",
    "        '5/17/2564', '7/11/2564', '10/7/2564', '11/26/2564', '3/30/2564', '1/26/2565',\n",
    "        '20/03/65', '25/03/65', '29/03/65', '3/5/2565', '6/6/2565', 'รายใหม่',\n",
    "        '11/6/2565', '18/07/2565', '26/07/2565', '7/8/2565', '-', '24/09/2565',\n",
    "        '30/09/2565', '30/11/2565', '26/12/2565', '08/02/2561 - 07/02/2566',\n",
    "        '13/06/2561 - 12/06/2566', '25/06/2561 - 24/06/2566', '19/07/2561 - 18/07/2566',\n",
    "        '24/07/2561 - 23/07/2566', 'nan', '24/08/2561 - 23/08/2566', '03/09/2561 - 02/09/2566',\n",
    "        '28/09/2561 - 27/09/2566', '21/10/2561 - 20/10/2566', '03/12/2561 - 02/12/2566',\n",
    "        '22/12/2561 - 21/12/2566', 'nan', '20/04/2562 - 19/04/2567', '16/05/2562 - 15/05/2567',\n",
    "        '06/06/2562 - 05/06/2567', '21/05/2562 - 20/05/2567', 'nan', '30/07/2562 - 29/07/2567',\n",
    "        '02/09/2562 - 01/09/2567', 'nan', '05/09/2562 - 04/09/2567', '20/10/2562 - 19/10/2567',\n",
    "        '31/10/2562 - 30/10/2567', '30/11/2562 - 29/11/2567', '12/12/2562 - 11/12/2567',\n",
    "        'nan', '06/02/2563 - 05/02/2568', '31/03/2563 - 30/03/2568', '22/04/2563 - 21/04/2568',\n",
    "        '08/04/2563 - 07/04/2568', 'nan', '06/05/2563 - 05/05/2568', 'nan', 'nan',\n",
    "        '31/07/2563 - 30/07/2568', '02/07/2563 - 01/07/2568', 'nan', 'nan', 'nan', 'nan',\n",
    "        'nan', 'nan', 'nan', 'nan', 'nan'\n",
    "    ]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "def clean_and_split_dates_to_ad(df, col_name='วันครบอายุเห็นชอบ'):\n",
    "    s = df[col_name].astype(str).str.strip()\n",
    "    mask = s.str.contains(' - ', na=False)\n",
    "    \n",
    "    split_data = s[mask].str.split(' - ', n=1, expand=True)\n",
    "    \n",
    "    df['วันที่อนุญาต'] = pd.Series(dtype='object')\n",
    "    if not split_data.empty:\n",
    "        df.loc[mask, 'วันที่อนุญาต'] = split_data[0]\n",
    "        df.loc[mask, col_name] = split_data[1]\n",
    "\n",
    "    def convert_thai_dates_to_ad(series):\n",
    "        dates = series.astype(str).str.strip().replace(['___', '-', 'nan', 'รายใหม่'], np.nan)\n",
    "        clean_dates = dates.dropna()\n",
    "        \n",
    "        if clean_dates.empty:\n",
    "            return pd.Series(pd.NaT, index=series.index)\n",
    "            \n",
    "        parts = clean_dates.str.split('/', expand=True)\n",
    "        year = pd.to_numeric(parts[parts.columns[-1]], errors='coerce')\n",
    "        ad_year = np.where(year < 100, year + 2500, year) - 543\n",
    "        \n",
    "        temp_df = pd.DataFrame({\n",
    "            'year': ad_year,\n",
    "            'month': pd.to_numeric(parts[1], errors='coerce'),\n",
    "            'day': pd.to_numeric(parts[0], errors='coerce')\n",
    "        })\n",
    "        \n",
    "        converted_dates = pd.to_datetime(temp_df, errors='coerce')\n",
    "        return converted_dates.reindex(series.index)\n",
    "\n",
    "    df['วันที่อนุญาต'] = convert_thai_dates_to_ad(df['วันที่อนุญาต'])\n",
    "    df[col_name] = convert_thai_dates_to_ad(df[col_name])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def convert_ad_to_be_str(ad_date_series):\n",
    "    # This function converts a pandas Series of AD datetime objects to BE date strings\n",
    "    be_series = ad_date_series.copy()\n",
    "    valid_dates_mask = be_series.notna()\n",
    "    \n",
    "    # Apply conversion only on valid (non-NaT) dates\n",
    "    be_series.loc[valid_dates_mask] = (\n",
    "        be_series.loc[valid_dates_mask] + pd.DateOffset(years=543)\n",
    "    ).dt.strftime('%d/%m/%Y')\n",
    "    \n",
    "    return be_series\n",
    "\n",
    "# --- Main Execution ---\n",
    "\n",
    "# 1. Clean data and convert dates to AD for processing\n",
    "df_processed = clean_and_split_dates_to_ad(df.copy())\n",
    "\n",
    "# 2. Create a new DataFrame for export\n",
    "df_for_export = df_processed.copy()\n",
    "\n",
    "# 3. Convert date columns back to BE format for the export file\n",
    "df_for_export['วันที่อนุญาต'] = convert_ad_to_be_str(df_for_export['วันที่อนุญาต'])\n",
    "df_for_export['วันครบอายุเห็นชอบ'] = convert_ad_to_be_str(df_for_export['วันครบอายุเห็นชอบ'])\n",
    "\n",
    "# 4. Define the output filename\n",
    "output_filename = \"FA_Data_Cleaned_BE.xlsx\"\n",
    "\n",
    "# 5. Save the final DataFrame (with BE dates) to an Excel file\n",
    "df_for_export.to_excel(\n",
    "    output_filename,\n",
    "    sheet_name='Cleaned Data (BE)',\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(f\"Data successfully cleaned and saved to '{output_filename}' with dates in BE format.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1043d525",
   "metadata": {},
   "source": [
    "### FA-2 Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49959859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_file = \"Dataset\\FA-2 (ปี 2565).xlsm\"\n",
    "output_file = \"FA-2 Sheet.xlsx\"\n",
    "\n",
    "data = pd.read_excel(input_file, dtype=str)\n",
    "\n",
    "p_columns = [\"รายใหม่\", \"ต่ออายุ\"]\n",
    "is_p_or_1 = data[p_columns].isin([\"P\", \"1\"])\n",
    "mask = is_p_or_1.any(axis=1)\n",
    "\n",
    "data.loc[mask, 'ประเภทคำขอ'] = is_p_or_1[mask].idxmax(axis=1)\n",
    "\n",
    "data.to_excel(\n",
    "    output_file,\n",
    "    sheet_name='Processed_Data',\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c45f6ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_22724\\774214242.py:9: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['สถิติ'] = pd.to_datetime(df[\"วันที่ยื่นคำขอ\"], errors='coerce').dt.quarter.apply(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "input_file = \"Dataset\\FA-2 (ปี 2565).xlsm\"\n",
    "output_file = \"FA-2_Processed.xlsx\"\n",
    "\n",
    "df = pd.read_excel(input_file, dtype=str)\n",
    "\n",
    "df['สถิติ'] = pd.to_datetime(df[\"วันที่ยื่นคำขอ\"], errors='coerce').dt.quarter.apply(\n",
    "    lambda q: f'Quarter {int(q)}' if pd.notna(q) else ''\n",
    ")\n",
    "\n",
    "app_type_cols = df.columns.intersection([\"รายใหม่\", \"ต่ออายุ\"])\n",
    "if not app_type_cols.empty:\n",
    "    is_p_or_1 = df[app_type_cols].isin([\"P\", \"1\"])\n",
    "    mask = is_p_or_1.any(axis=1)\n",
    "    df.loc[mask, 'ประเภทคำขอ'] = is_p_or_1[mask].idxmax(axis=1)\n",
    "\n",
    "cols = df.columns.tolist()\n",
    "try:\n",
    "    # Find insertion point after the column containing 'FA-2'\n",
    "    ref_col_index = next(i for i, col in enumerate(cols) if 'FA-2' in col)\n",
    "    # Move 'สถิติ' column to the correct position\n",
    "    cols.insert(ref_col_index + 1, cols.pop(cols.index('สถิติ')))\n",
    "    df = df[cols]\n",
    "except (StopIteration, ValueError):\n",
    "    # If reference column isn't found, the order remains as is\n",
    "    pass\n",
    "\n",
    "df.to_excel(output_file, sheet_name='Processed_Data', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8caefc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15226061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ประมวลผลเสร็จสิ้น บันทึกไฟล์ที่: Dataset\\FA-2 Sheet_classified_llm_final.xlsx\n",
      "\n",
      "ตัวอย่างผลลัพธ์:\n",
      "                       ชื่อบริษัท FA  คำนำหน้า\n",
      "0                    เคที ซีมิโก้ บล.       บล\n",
      "1       เมอร์ชั่น พาร์ทเนอร์ บล. บมจ.       บล\n",
      "2              แอพเพิล เวลธ์ บล. บมจ.       บล\n",
      "3              เพลินจิต แคปปิตอล บจก.      บจก\n",
      "4             เวลแคป แอดไวเซอรี่ บจก.      บจก\n",
      "5             เอส 14 แอดไวเซอรี่ บจก.      บจก\n",
      "6                     เออีซี บล. บมจ.       บล\n",
      "7            ออพท์เอเชีย แคปิตอล บจก.      บจก\n",
      "8        หยวนต้า (ประเทศไทย) บล. บมจ.       บล\n",
      "9        หยวนต้า (ประเทศไทย) บล. บมจ.       บล\n",
      "10                  โกลเบล็ก บล. บจก.       บล\n",
      "11                    กรุงไทย ธ. บมจ.   ธนาคาร\n",
      "12        แอสเซท โปร แมเนจเม้นท์ บจก.      บจก\n",
      "13  แคปปิตอล ลิ้งค์ แอ๊ดไวเซอรี่ บจก.      บจก\n",
      "14         ที่ปรึกษา เอเชีย พลัส บจก.      บจก\n",
      "15  แคปปิตอล ลิ้งค์ แอ๊ดไวเซอรี่ บจก.      บจก\n",
      "16        เคทีบี (ประเทศไทย) บล. บมจ.       บล\n",
      "17                    กรุงเทพ ธ. บมจ.   ธนาคาร\n",
      "18           สยาม อัลฟา แคปปิตอล บจก.      บจก\n",
      "19                   เซจแคปปิตอล บจก.      บจก\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "import random\n",
    "\n",
    "API_KEY = \"sk-GqA4Uj6iZXaykbOzIlFGtmdJr6VqiX94NhhjPZaf81kylRzh\"\n",
    "BASE_URL = \"https://api.opentyphoon.ai/v1\"\n",
    "MODEL = \"typhoon-v2.1-12b-instruct\"\n",
    "FILE_PATH = \"Dataset\\FA-2 Sheet.xlsx\"\n",
    "OUTPUT_FILE_PATH = \"Dataset\\FA-2 Sheet_classified_llm_final.xlsx\"\n",
    "COMPANY_COLUMN = 'ชื่อบริษัท FA '\n",
    "PREFIX_COLUMN = 'คำนำหน้า'\n",
    "VALID_TYPES = {'บจก', 'บล', 'ธนาคาร', 'บลจ'}\n",
    "SYSTEM_PROMPT = (\n",
    "    \"คุณคือ AI ผู้เชี่ยวชาญการจำแนกประเภทบริษัทในไทย \"\n",
    "    \"ให้ตอบกลับมาเพียงคำเดียวจากตัวเลือกนี้: 'บจก.', 'บล.', 'ธนาคาร', 'บลจ.' \"\n",
    "    \"ถ้าไม่เข้าข่าย ให้ตอบ 'Unknown'\"\n",
    ")\n",
    "\n",
    "client = OpenAI(api_key=API_KEY, base_url=BASE_URL)\n",
    "\n",
    "def classify(company_name):\n",
    "    if pd.isna(company_name):\n",
    "        return \"\"\n",
    "    for i in range(5):\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": f\"'{company_name}'\"}\n",
    "            ],\n",
    "            max_tokens=10,\n",
    "            temperature=0.0\n",
    "        )\n",
    "        result = response.choices[0].message.content.strip().replace('.', '')\n",
    "        if result in VALID_TYPES:\n",
    "            return result\n",
    "        if i < 4:\n",
    "            time.sleep((2 ** i) + random.uniform(0, 1))\n",
    "    return \"Unknown\"\n",
    "\n",
    "df = pd.read_excel(FILE_PATH)\n",
    "\n",
    "if PREFIX_COLUMN not in df.columns:\n",
    "    df[PREFIX_COLUMN] = \"\"\n",
    "\n",
    "names = df[COMPANY_COLUMN].dropna().unique().tolist()\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "    results = list(executor.map(classify, names))\n",
    "\n",
    "mapping = dict(zip(names, results))\n",
    "df[PREFIX_COLUMN] = df[COMPANY_COLUMN].map(mapping).fillna('')\n",
    "df.to_excel(OUTPUT_FILE_PATH, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac3071b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing names:   0%|          | 0/517 [00:00<?, ?it/s]Task exception was never retrieved\n",
      "future: <Task finished name='Task-3' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 102 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 102 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-4' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 110 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 110 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-5' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 95 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 95 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-6' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 138 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 138 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-7' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 95 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 95 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-8' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 96 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 96 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-9' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 96 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 96 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-10' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 96 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 96 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-11' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 93 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 93 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-12' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-13' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-14' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-15' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-16' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-17' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-18' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-19' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-20' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-21' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-23' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-24' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-26' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-28' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-29' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-30' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-31' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-32' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-34' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-35' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-36' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-37' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-38' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-39' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-40' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-41' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-42' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-43' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-44' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-45' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-46' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-47' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-48' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-49' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-50' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 94 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 94 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-51' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 113 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 113 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-52' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 94 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 94 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-53' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 93 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 93 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-54' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 91 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 91 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-55' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 94 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 94 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-56' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 98 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 98 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-57' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 96 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 96 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-58' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 94 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 94 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-59' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 94 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 94 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-60' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-61' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-62' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-63' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-64' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-65' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-66' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-67' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-68' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-69' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-70' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-71' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-72' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-73' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-74' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-75' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-76' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-77' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-78' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-79' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-80' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-81' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-82' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-83' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-84' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-85' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-86' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-87' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-88' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-89' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-90' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-91' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-92' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-93' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-94' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 94 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 94 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-95' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 94 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 94 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-96' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 93 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 93 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-97' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 94 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 94 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-98' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 93 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 93 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-99' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 94 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 94 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-100' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 93 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 93 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-101' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 93 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 93 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-102' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 96 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 96 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-103' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 95 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 95 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-104' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-105' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-106' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-107' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-108' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-109' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-110' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-111' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-112' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-113' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-114' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-115' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-116' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-117' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-118' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-119' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-120' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-121' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-122' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-123' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-124' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-125' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-126' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-127' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-128' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-129' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-130' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-131' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-132' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-133' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-134' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-135' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-136' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-137' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-138' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 92 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 92 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-139' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 92 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 92 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-140' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 100 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 100 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-141' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 94 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 94 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-142' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 92 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 92 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-143' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 120 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 120 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-144' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 93 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 93 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-145' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 97 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 97 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-146' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 100 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 100 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-147' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 94 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 94 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-148' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-149' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-150' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-151' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-152' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-153' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-154' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-155' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-156' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-157' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-158' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-159' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-160' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-161' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-162' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-163' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-164' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-165' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-166' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-167' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-168' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-169' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-170' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-171' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-172' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-173' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-174' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-175' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-176' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-177' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-178' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-179' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-180' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-182' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 99 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 99 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-183' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 92 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 92 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-185' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 104 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 104 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-186' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 103 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 103 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-188' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 107 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 107 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-189' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 99 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 99 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-190' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 112 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 112 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-191' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 95 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 95 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-192' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 100 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 100 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-193' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 98 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 98 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-194' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-195' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-196' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-197' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-198' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-199' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-200' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-201' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-202' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-203' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-204' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-205' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-206' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-207' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-208' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-209' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-210' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-211' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-212' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-213' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-214' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-215' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-216' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-217' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-218' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-219' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-220' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-221' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-222' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-223' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-224' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-225' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-226' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-227' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 102 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 102 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-228' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 94 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 94 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-229' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 95 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 95 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-230' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 98 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 98 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-231' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 92 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 92 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-232' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 95 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 95 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-233' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 101 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 101 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-234' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 93 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 93 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-235' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 94 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 94 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-236' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 115 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 115 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-237' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-238' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-239' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-240' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-241' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-242' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-243' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-244' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-245' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-246' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-247' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-248' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-249' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-250' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-251' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-252' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-253' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-254' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-255' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-256' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-257' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-258' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-259' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-260' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-261' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-262' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-263' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-264' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-265' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-267' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-268' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-269' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-270' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-271' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 96 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 96 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-272' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 103 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 103 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-273' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 94 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 94 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-274' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 101 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 101 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-275' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 96 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 96 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-276' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 111 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 111 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-277' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 121 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 121 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-278' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 115 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 115 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-279' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 110 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 110 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-280' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 98 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 98 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-281' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-282' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-283' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-284' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-285' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-286' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-287' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-288' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-289' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-290' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-291' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-292' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-293' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-294' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-295' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-296' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-297' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-298' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-299' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-300' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-301' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-302' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-303' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-304' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-305' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-306' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-307' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-308' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-309' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-310' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-311' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-312' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-313' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-314' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-315' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 95 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 95 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-316' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 105 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 105 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-317' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 97 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 97 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-318' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 97 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 97 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-319' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 93 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 93 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-320' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 99 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 99 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-321' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 111 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 111 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-322' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 92 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 92 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-323' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 97 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 97 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-324' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 94 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 94 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-325' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-326' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-327' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-328' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-329' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-330' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-331' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-332' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-333' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-334' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-335' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-336' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-337' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-338' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-339' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-340' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-341' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-342' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-343' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-344' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-346' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-347' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-348' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-349' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-351' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-352' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-353' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-354' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-355' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-356' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-357' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-358' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-359' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-360' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-361' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 102 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 102 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-362' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 91 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 91 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-363' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 112 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 112 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-364' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 97 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 97 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-365' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 98 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 98 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-366' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 100 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 100 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-367' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 96 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 96 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-368' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 97 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 97 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-369' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 110 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 110 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-370' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 93 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 93 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-371' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-372' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-373' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-374' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-375' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-376' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-377' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-378' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-379' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-380' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-381' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-382' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-383' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-384' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-385' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-386' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-387' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-388' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-389' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-390' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-391' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-392' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-393' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-394' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-395' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-396' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-397' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-398' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-399' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-400' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-401' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-402' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-403' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-404' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 95 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 95 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-405' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 102 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 102 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-406' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 95 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 95 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-407' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 96 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 96 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-409' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 91 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 91 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-410' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 95 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 95 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-411' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 92 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 92 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-412' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 95 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 95 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-413' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 94 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 94 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-414' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 91 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 91 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-415' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-416' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-417' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-418' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-419' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-420' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-421' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-422' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-423' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-424' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-425' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-426' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-427' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-428' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-429' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-430' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-431' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-432' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-433' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-434' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-435' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-436' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-437' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-438' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-439' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-440' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-441' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-443' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-444' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-445' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-447' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-448' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-449' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-450' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 95 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 95 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-451' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 94 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 94 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-452' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 107 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 107 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-453' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 101 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 101 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-454' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 111 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 111 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-455' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 97 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 97 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-456' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 91 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 91 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-457' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 110 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 110 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-458' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 97 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 97 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-459' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 103 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 103 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-460' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-461' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-462' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-463' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-464' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-465' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-466' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-467' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-468' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-469' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-470' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-471' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-472' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-473' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-474' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-475' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-476' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-478' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-479' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-480' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-482' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-483' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-484' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-485' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-486' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-487' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-488' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-489' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-490' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-491' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-492' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-493' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 90 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 90 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-494' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 101 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 101 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-495' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 96 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 96 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-496' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 96 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 96 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-497' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 96 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 96 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-498' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 109 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 109 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-499' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 96 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 96 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-500' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 90 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 90 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-501' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 93 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 93 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-502' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=BadRequestError(\"Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 98 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'detail': {'error': 'litellm.BadRequestError: OpenAIException - Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4049. Given: 98 `inputs` tokens and 8000 `max_new_tokens`', 'context': {'status_code': 400}}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-503' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-504' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-505' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-506' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-507' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-508' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-509' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-510' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-511' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-512' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-513' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-514' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-515' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-516' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-517' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-518' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=RateLimitError('Error code: 429 - {\\'detail\\': \"You\\'ve hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"C:\\Users\\kongl\\AppData\\Local\\Temp\\ipykernel_4632\\1600588531.py\", line 23, in extract_names_async\n",
      "    response = await client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kongl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'detail': \"You've hit the rate limit for this service. For higher request limits, consider using the Typhoon model at https://api.together.ai/models.\"}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import asyncio\n",
    "from tqdm.asyncio import tqdm as anic_tqdm\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "API_KEY = (\"sk-GqA4Uj6iZXaykbOzIlFGtmdJr6VqiX94NhhjPZaf81kylRzh\")\n",
    "FILE_PATH = (\"Dataset\\FA-2 (ปี 2565)(Sheet1).csv\")\n",
    "OUTPUT_PATH = \"FA-2565_cleaned_and_exploded_minimal.csv\"\n",
    "TARGET_COLUMN_NAME = \"ให้ความเห็นชอบผู้ควบคุมฯ (แบบ FA-2)\"\n",
    "SYSTEM_PROMPT = \"You are an efficient data cleaning assistant. Your task is to extract only Thai human names. Return ONLY the names, each on a new line. Provide no other text or formatting.\"\n",
    "USER_PROMPT_TEMPLATE = \"Extract all human names from this text:\\n\\n---\\n{}\\n---\"\n",
    "\n",
    "client = OpenAI(api_key=API_KEY, base_url=\"https://api.opentyphoon.ai/v1\")\n",
    "\n",
    "async def extract_names_async(text_to_clean: str) -> list[str]:\n",
    "    if pd.isna(text_to_clean) or not str(text_to_clean).strip():\n",
    "        return []\n",
    "    \n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"typhoon-v2.1-12b-instruct\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": USER_PROMPT_TEMPLATE.format(text_to_clean)}\n",
    "        ],\n",
    "        max_tokens=32784,\n",
    "        temperature=0.1,\n",
    "        timeout=30.0\n",
    "    )\n",
    "    return [name.strip() for name in response.choices[0].message.content.split('\\n') if name.strip()]\n",
    "\n",
    "async def main():\n",
    "    df = pd.read_csv(FILE_PATH)\n",
    "    tasks = [extract_names_async(text) for text in df[TARGET_COLUMN_NAME]]\n",
    "    extracted_names = await anic_tqdm.gather(*tasks, desc=\"Processing names\")\n",
    "\n",
    "    (df.assign(cleaned_names=extracted_names)\n",
    "       .explode('cleaned_names')\n",
    "       .assign(**{TARGET_COLUMN_NAME: lambda x: x['cleaned_names'].fillna('')})\n",
    "       .drop(columns=['cleaned_names'])\n",
    "       .to_csv(OUTPUT_PATH, index=False, encoding='utf-8-sig'))\n",
    "\n",
    "    print(f\"Process complete. Output saved to {OUTPUT_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
